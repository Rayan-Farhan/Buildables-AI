<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Bot</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: white;
        }
        
        .container {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.37);
        }
        
        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.2em;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        #controls { text-align:center; margin-bottom:18px; }
        button {
            padding: 12px 26px;
            font-size: 15px;
            border: none;
            border-radius: 50px;
            cursor: pointer;
            transition: all 0.2s ease;
            font-weight: bold;
            background: linear-gradient(45deg,#4CAF50,#45a049);
            color: white;
        }
        
        #stopBtn {
            background: linear-gradient(45deg,#f44336,#da190b);
            margin-left:10px;
        }
        
        .status {
            text-align: center;
            margin: 12px 0;
            padding: 12px;
            border-radius: 10px;
            font-weight: bold;
            background: rgba(33, 150, 243, 0.12);
            border: 2px solid rgba(33,150,243,0.2);
        }
        
        .conversation {
            background: rgba(255, 255, 255, 0.06);
            border-radius: 15px;
            padding: 20px;
            margin-top: 20px;
            max-height: 430px;
            overflow-y: auto;
        }
        
        .message {
            margin: 10px 0;
            padding: 10px 15px;
            border-radius: 15px;
            max-width: 80%;
            word-wrap: break-word;
        }
        
        .user-message {
            background: rgba(33, 150, 243, 0.22);
            margin-left: auto;
            text-align: right;
        }
        
        .bot-message {
            background: rgba(76, 175, 80, 0.18);
            margin-right: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Your Personal Chatty Bot üî•</h1>
        <div id="controls">
            <button id="startBtn">Start Chat</button>
            <button id="stopBtn" style="display:none;">End Chat</button>
        </div>
        
        <div id="status" class="status">
            Click "Start Chat" once and allow the mic when prompted. Then speak; I'll detect when you're done.
        </div>
        
        <div id="conversation" class="conversation">
            <div class="message bot-message">
                üëã Hello. Your personal chatty bot here!
            </div>
        </div>
    </div>

<script>
class VoiceBot {
    constructor() {
        // audio / stream state
        this.stream = null;
        this.mediaRecorder = null;
        this.audioChunks = [];
        this.audioContext = null;
        this.analyser = null;
        this.dataArray = null;
        this.rafId = null;

        // logic flags
        this.isRecording = false;       // true while mediaRecorder is recording
        this.isPlaying = false;         // true while bot audio is playing
        this.speaking = false;          // true while user is considered speaking
        this.suspendMonitoring = false; // true when we shouldn't start recordings (processing/playing)
        this.lastVoiceTime = 0;
        this.speechStartTime = 0;

        // parameters (tunables)
        this.rmsThreshold = 0.02;         // voice detection threshold (RMS)
        this.silenceTimeout = 1100;       // ms of silence to consider end of utterance
        this.minSpeakTime = 220;          // minimum ms of speech required to send
        this.fftSize = 2048;

        // DOM
        this.startBtn = document.getElementById('startBtn');
        this.stopBtn = document.getElementById('stopBtn');
        this.statusEl = document.getElementById('status');
        this.conversation = document.getElementById('conversation');

        // events
        this.startBtn.addEventListener('click', () => this.startChat());
        this.stopBtn.addEventListener('click', () => this.stopChat());

        // cleanup on unload
        window.addEventListener('beforeunload', () => this._cleanupTracks());
    }

    async startChat() {
        try {
            this.startBtn.disabled = true;
            this.updateStatus('Requesting microphone access...');

            if (!this.stream) {
                this.stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 16000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });
            }

            // show stop button
            this.stopBtn.style.display = 'inline-block';
            this.startBtn.style.display = 'none';

            // create audio context + analyser only once
            if (!this.audioContext) {
                this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = this.audioContext.createMediaStreamSource(this.stream);
                this.analyser = this.audioContext.createAnalyser();
                this.analyser.fftSize = this.fftSize;
                source.connect(this.analyser);
                this.dataArray = new Uint8Array(this.analyser.fftSize);
            }

            this.updateStatus('üéß Listening (I will detect when you stop speaking)...');
            this._startMonitorLoop();
        } catch (err) {
            console.error('Microphone error', err);
            this.updateStatus('‚ùå Microphone access denied or error. Check permissions and reload.');
            this.startBtn.disabled = false;
        }
    }

    stopChat() {
        this.updateStatus('Chat ended. You can refresh to start again.');
        this.stopBtn.style.display = 'none';
        this.startBtn.style.display = 'inline-block';
        this.startBtn.disabled = false;
        this._cleanupTracks();
        this._stopMonitorLoop();
    }

    _cleanupTracks() {
        try {
            if (this.stream) {
                this.stream.getTracks().forEach(t => t.stop());
                this.stream = null;
            }
            if (this.audioContext) {
                try { this.audioContext.close(); } catch (e) {}
                this.audioContext = null;
            }
        } catch (e) {
            console.warn('Error cleaning up tracks', e);
        }
    }

    _startMonitorLoop() {
        if (this.rafId) return;
        const monitor = () => {
            if (!this.analyser || this.suspendMonitoring) {
                this.rafId = requestAnimationFrame(monitor);
                return;
            }

            this.analyser.getByteTimeDomainData(this.dataArray);
            // compute normalized RMS from byte data
            let sum = 0;
            for (let i = 0; i < this.dataArray.length; i++) {
                const v = (this.dataArray[i] - 128) / 128;
                sum += v * v;
            }
            const rms = Math.sqrt(sum / this.dataArray.length);

            const now = Date.now();

            if (rms > this.rmsThreshold) {
                // voice present
                this.lastVoiceTime = now;
                if (!this.speaking && !this.isRecording && !this.isPlaying) {
                    // start recording
                    this._startMediaRecorder();
                    this.speaking = true;
                    this.speechStartTime = now;
                    this.updateStatus('üéôÔ∏è Recording ‚Äî speak now');
                }
            }

            if (this.speaking) {
                // check silence after lastVoiceTime
                if (now - this.lastVoiceTime > this.silenceTimeout) {
                    const spokenFor = now - this.speechStartTime;
                    // only stop if we've recorded at least minSpeakTime
                    if (this.isRecording && spokenFor > this.minSpeakTime) {
                        this.speaking = false;
                        this._stopMediaRecorder();
                        this.updateStatus('‚è≥ Processing your message...');
                    } else if (this.isRecording && spokenFor <= this.minSpeakTime) {
                        // too short -> discard (silence/false trigger)
                        this.speaking = false;
                        this._stopMediaRecorder(true); // pass discard flag
                        this.updateStatus('‚ö†Ô∏è Too short, waiting...');
                    }
                }
            }

            this.rafId = requestAnimationFrame(monitor);
        };

        this.rafId = requestAnimationFrame(monitor);
    }

    _stopMonitorLoop() {
        if (this.rafId) {
            cancelAnimationFrame(this.rafId);
            this.rafId = null;
        }
    }

    _startMediaRecorder() {
        if (!this.stream) return;
        try {
            this.audioChunks = [];
            // create a new mediaRecorder on the existing stream (do NOT stop tracks)
            this.mediaRecorder = new MediaRecorder(this.stream, { mimeType: 'audio/webm;codecs=opus' });
            this.mediaRecorder.ondataavailable = (e) => {
                if (e.data && e.data.size > 0) this.audioChunks.push(e.data);
            };
            this.mediaRecorder.onstop = () => {
                // handle after a short tick so that any remaining dataavailable events arrived
                setTimeout(() => this._handleRecordingStop(), 50);
            };
            this.mediaRecorder.start();
            this.isRecording = true;
        } catch (err) {
            console.error('MediaRecorder start failed', err);
            this.updateStatus('‚ùå Unable to start recording. Browser may not support MediaRecorder with opus/webm.');
        }
    }

    _stopMediaRecorder(discard=false) {
        try {
            if (this.mediaRecorder && this.isRecording && this.mediaRecorder.state === 'recording') {
                // store discard flag so handler knows
                this._discardCurrentChunk = !!discard;
                this.mediaRecorder.stop();
            }
        } catch (e) {
            console.warn('Error stopping recorder', e);
        } finally {
            this.isRecording = false;
        }
    }

    async _handleRecordingStop() {
        // if flagged to discard (too short), flush and return to listening
        if (this._discardCurrentChunk) {
            this._discardCurrentChunk = false;
            this.audioChunks = [];
            this.updateStatus('Listening...');
            return;
        }

        // combine data
        const recordedBlob = new Blob(this.audioChunks, { type: 'audio/webm' });

        // small safety check: don't send tiny blobs
        if (recordedBlob.size < 500) {
            // probably noise
            this.updateStatus('Listening...');
            this.audioChunks = [];
            return;
        }

        // suspend monitoring while we upload/process/play
        this.suspendMonitoring = true;
        this.updateStatus('‚è≥ Sending to server for transcription and response...');

        try {
            const formData = new FormData();
            formData.append('audio_file', recordedBlob, 'utterance.webm');

            const res = await fetch('http://localhost:8000/process-audio', {
                method: 'POST',
                body: formData
            });

            if (!res.ok) {
                const txt = await res.text().catch(()=>null);
                throw new Error(`Server error ${res.status} ${txt||''}`);
            }

            const data = await res.json();
            // show user transcription
            this._appendMessage('user', `üë§ ${data.user_transcription || '(no transcription)'} `);

            // play TTS
            await this._playBotReply(data.audio_data, data.bot_response || '');

        } catch (err) {
            console.error('Processing error', err);
            this.updateStatus('‚ùå Error processing audio. See console.');
            // resume monitoring after small pause
            setTimeout(()=> {
                this.suspendMonitoring = false;
                this.updateStatus('Listening...');
            }, 900);
        } finally {
            this.audioChunks = [];
        }
    }

    async _playBotReply(audioBase64, botText) {
        // Convert base64 -> binary -> blob
        try {
            this.isPlaying = true;
            this.updateStatus('üîä Playing reply...');

            // append bot text message first (so user sees text while audio plays)
            if (botText && botText.trim().length > 0) {
                this._appendMessage('bot', `ü§ñ ${botText}`);
            }

            // decode audio base64
            const byteCharacters = atob(audioBase64);
            const byteNumbers = new Array(byteCharacters.length);
            for (let i = 0; i < byteCharacters.length; i++) {
                byteNumbers[i] = byteCharacters.charCodeAt(i);
            }
            const byteArray = new Uint8Array(byteNumbers);
            const audioBlob = new Blob([byteArray], { type: 'audio/mpeg' });
            const audioUrl = URL.createObjectURL(audioBlob);
            const audioEl = new Audio();
            audioEl.src = audioUrl;
            audioEl.autoplay = true;

            // while playing we keep suspendMonitoring = true (set earlier)
            await new Promise((resolve) => {
                audioEl.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    resolve();
                };
                audioEl.onerror = (e) => {
                    console.warn('TTS playback error', e);
                    URL.revokeObjectURL(audioUrl);
                    resolve();
                };
                // start playback (some browsers require user gesture ‚Äî Start Chat click is that user gesture)
                audioEl.play().catch(err => {
                    console.warn('Audio play failed', err);
                    resolve();
                });
            });

            // playback finished, resume listening after a short friendly delay
            this.isPlaying = false;
            this.suspendMonitoring = false;
            this.updateStatus('‚úÖ Your turn ‚Äî speak when ready.');
            // small pause to avoid immediate retrigger from leftover noise
            setTimeout(()=> {
                // ensure monitor loop is running
                if (!this.rafId) this._startMonitorLoop();
            }, 250);

        } catch (err) {
            console.error('playBotReply error', err);
            this.isPlaying = false;
            this.suspendMonitoring = false;
            this.updateStatus('‚ö†Ô∏è Error playing reply. Listening again...');
        }
    }

    _appendMessage(type, text) {
        const div = document.createElement('div');
        div.className = `message ${type}-message`;
        div.textContent = text;
        this.conversation.appendChild(div);
        this.conversation.scrollTop = this.conversation.scrollHeight;
    }

    updateStatus(text) {
        this.statusEl.textContent = text;
    }
}

document.addEventListener('DOMContentLoaded', () => {
    new VoiceBot();
});
</script>
</body>
</html>